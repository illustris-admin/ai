# -*- coding: utf-8 -*-
"""Query a PDF With Your Local AI in Google Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uZvBBG65O5P6oNqquXxfW1SmgC4UX5JC

### ğŸ“˜ Video 3: Query a PDF (Ask Questions About Any Document)

**Goal**: Upload a PDF and ask questions â€” teach your AI to read and understand documents.

âœ… No API keys

âœ… Uses Hugging Face + LangChain

âœ… Runs on free Colab GPU

ğŸ“„ Let's make AI work for *your* data!
"""

# Install required libraries
!pip install -q langchain-huggingface langchain-community transformers torch accelerate bitsandbytes pdfplumber

"""### ğŸ“š Why Query a PDF?

You have documents:
- Research papers
- Contracts
- Manuals
- Articles

But who has time to read them?

ğŸ‘‰ Teach your AI to **read, remember, and answer** â€” privately and for free.

ğŸ’¡ This is **RAG (Retrieval-Augmented Generation)** â€” a core pattern in Doug Ortizâ€™s AI & LLM work.
"""

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_huggingface import HuggingFacePipeline

# Load lightweight model
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto",
    trust_remote_code=True
)

# Create text generation pipeline
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=100,
    temperature=0.7,
    do_sample=True
)

# Wrap with LangChain
llm = HuggingFacePipeline(pipeline=pipe)

print("âœ… Model loaded and ready!")

"""### ğŸ“ Upload a PDF

Click below to upload any PDF (e.g., a research paper, manual, or article).
"""

from google.colab import files

uploaded = files.upload()
pdf_path = list(uploaded.keys())[0]
print(f"Uploaded: {pdf_path}")

"""### ğŸ§© Extract Text from PDF"""

import pdfplumber

# Extract text from PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            extracted = page.extract_text()
            if extracted:
                text += extracted + "\n"
    return text

raw_text = extract_text_from_pdf(pdf_path)

# Use first ~2000 chars to fit context and keep it fast
context = raw_text[:2000]

print(f"âœ… Extracted {len(raw_text)} characters. Using first 2000 for speed.")
print(f"Preview: {context[:300]}...\n")

"""### ğŸ’¬ Ask Questions About the PDF"""

def query_pdf(question, context):
    prompt = f"<|im_start|>system\nYou are a helpful assistant. Answer based only on the provided text. Be concise.\n<|im_end|>\n<|im_start|>user\nDocument excerpt:\n{context}\n\nQuestion: {question}<|im_end|>\n<|im_start|>assistant\n"
    response = llm.invoke(prompt)
    # Clean response
    answer = response.replace(prompt, "").replace("<|im_start|>assistant\n", "").replace("<|im_end|>", "").strip()
    return answer

# Try a question
question = "What is this document about?"
answer = query_pdf(question, context)

print(f"Q: {question}")
print(f"A: {answer}\n")

# Another example
question2 = "Summarize the main points in 2 sentences."
answer2 = query_pdf(question2, context)

print(f"Q: {question2}")
print(f"A: {answer2}")

"""### ğŸ‰ Summary

In this notebook, you:

âœ… Uploaded a PDF

âœ… Extracted text using `pdfplumber`

âœ… Asked questions using your local LLM

ğŸ’¡ This is **RAG (Retrieval-Augmented Generation)** â€” a foundational AI pattern used in real-world systems

â¡ï¸ **Next: Give your AI web search â€” so it knows current events!**

### ğŸ”— Resources

- [LangChain Docs](https://python.langchain.com)
- [TinyLlama on Hugging Face](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
- [pdfplumber on PyPI](https://pypi.org/project/pdfplumber/)
- [Author: Doug Ortiz](https://www.linkedin.com/in/doug-ortiz-illustris/)
- [YouTube Channel: @techbits-do](https://www.youtube.com/@techbits-do)
- [Illustris.org](https://www.illustris.org)
"""