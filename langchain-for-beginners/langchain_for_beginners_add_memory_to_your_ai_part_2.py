# -*- coding: utf-8 -*-
"""LangChain_for_Beginners_Add_Memory_to_Your_AI_Part_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TYwyS4jby1VAsvi2usHTlQaxafpnuJI_

### 📘 LangChain for Beginners - Add Memory to Your AI - Part 2

**Goal**: Give your AI memory so it remembers what you say — build a chatbot that feels real.

✅ No API keys

✅ Uses Hugging Face + LangChain

✅ Runs on free Colab GPU

🧠 Let's make your AI remember — cleanly and clearly!
"""

# Install required libraries
!pip install -q langchain langchain-huggingface transformers torch accelerate bitsandbytes

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_huggingface import HuggingFacePipeline

# Load model
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto",
    trust_remote_code=True
)

# Create pipeline with clean generation settings
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=64,          # Keep responses short
    temperature=0.7,
    top_k=50,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id
)

# Wrap with LangChain
llm = HuggingFacePipeline(pipeline=pipe)

print("✅ Model loaded!")

"""### 🧩 Add Memory: Clean Conversation History"""

from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Simple prompt — no special tokens in template
template = """Past conversation:
{history}

Human: {input}
AI:"""

prompt = PromptTemplate(
    input_variables=["history", "input"],
    template=template
)

# Memory stores only clean input/output pairs
memory = ConversationBufferMemory(
    input_key="input",
    memory_key="history",
    format_messages=False  # Critical: avoids storing full prompt
)

# Create chain
conversation = LLMChain(
    llm=llm,
    prompt=prompt,
    memory=memory
)

"""### 💬 Clean Chat Demo: Watch the AI Remember"""

def chat(message):
    response = conversation.invoke({"input": message})
    return response['text'].strip()

print("🗨️  CHAT DEMO: AI with Memory\n")
print("This AI will remember your name and hobbies.\nWatch how it answers based on what you told it before.\n")
print("─" * 50)

# Message 1
user_msg = "My name is Alex."
print(f"🧑‍💻 You: {user_msg}")
ai_response = chat(user_msg)
print(f"🤖 AI: {ai_response}\n")

# Message 2
user_msg = "What's my name?"
print(f"🧑‍💻 You: {user_msg}")
ai_response = chat(user_msg)
print(f"🤖 AI: {ai_response}\n")

# Message 3
user_msg = "I like hiking and guitar."
print(f"🧑‍💻 You: {user_msg}")
ai_response = chat(user_msg)
print(f"🤖 AI: {ai_response}\n")

# Message 4
user_msg = "What hobbies did I mention?"
print(f"🧑‍💻 You: {user_msg}")
ai_response = chat(user_msg)
print(f"🤖 AI: {ai_response}")

print("\n" + "─" * 50)
print("✅ Success! The AI remembered your name and hobbies — that's memory in action.")

"""### 🎉 Summary

In this notebook, you:

✅ Added **memory** to your AI

✅ Used a **clean prompt template** (no token leakage)

✅ Built a chatbot that remembers names and hobbies

💡 Responses are **short, clear, and free of repetition**

➡️ **Next: Ask PDFs questions — teach your AI to read documents!**

### 🔗 Resources

- [LangChain Memory Docs](https://python.langchain.com/docs/modules/memory/)
- [TinyLlama on Hugging Face](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
- [Author: Doug Ortiz](https://www.linkedin.com/in/doug-ortiz-illustris/)
- [YouTube Channel: @techbits-do](https://www.youtube.com/@techbits-do)
- [Illustris.org](https://www.illustris.org)
"""