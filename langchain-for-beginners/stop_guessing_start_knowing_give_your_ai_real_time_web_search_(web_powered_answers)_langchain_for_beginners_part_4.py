# -*- coding: utf-8 -*-
"""Stop_Guessing_Start_Knowing-Give_Your_AI_Real-Time_Web_Search_(Web-Powered Answers)-LangChain for Beginners-Part_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iu3RIv10J3UOndUJjRRdGYgTx7LRthP4

### ğŸ“˜ Video 4: Make AI Search the Web (LangChain for Beginners, Part 4)

**Goal**: Let your AI search the web for up-to-date answers â€” no API keys, all free.

âœ… Uses DuckDuckGo (no API key)

âœ… Combines search + local LLM

âœ… Runs on free Colab GPU

ğŸŒ Let's make your AI know what's happening *right now*!
"""

# Install required libraries
!pip install -q langchain-huggingface langchain-community duckduckgo-search transformers torch accelerate bitsandbytes
!pip install -U ddgs

"""### ğŸŒ Why Add Web Search?

Local models like TinyLlama have **no knowledge after their training date**.

To answer:
- "Who won the latest Eurovision?"
- "What's NVIDIA's stock price today?"

â€¦you need **real-time data**.

ğŸ‘‰ This is how production AI systems stay current.
"""

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_huggingface import HuggingFacePipeline

# Load model
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto",
    trust_remote_code=True
)

# Create pipeline
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=100,
    temperature=0.7,
    do_sample=True
)

# Wrap with LangChain
llm = HuggingFacePipeline(pipeline=pipe)

print("âœ… Model loaded!")

"""### ğŸ” Add Web Search Tool"""

from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from langchain_core.tools import Tool

# Initialize search
search = DuckDuckGoSearchAPIWrapper()

# Define search tool
search_tool = Tool(
    name="web_search",
    description="Search the web for current information",
    func=lambda query: search.results(query, max_results=3)
)

"""### ğŸ§ª Ask Questions with Live Search"""

def answer_with_search(question):
    print(f"ğŸ” Searching: {question}")
    results = search_tool.run(question)

    # Format results
    context = "\n".join([f"[{i+1}] {r['title']}: {r['snippet']}" for i, r in enumerate(results)])

    prompt = f"<|im_start|>system\nYou are a helpful assistant. Answer based only on the search results below. Be concise.\n<|im_end|>\n<|im_start|>user\nSearch results:\n{context}\n\nQuestion: {question}<|im_end|>\n<|im_start|>assistant\n"
    response = llm.invoke(prompt)
    answer = response.replace(prompt, "").replace("<|im_start|>assistant\n", "").replace("<|im_end|>", "").strip()
    return answer

# Try it
q1 = "Who won the 2024 Eurovision Song Contest?"
print(f"Q: {q1}")
print(f"A: {answer_with_search(q1)}\n")

q2 = "What is the capital of Japan?"  # Simple fact
print(f"Q: {q2}")
print(f"A: {answer_with_search(q2)}")

q3 = "Where will the next soccer cup be held?"
print(f"Q: {q3}")
print(f"A: {answer_with_search(q3)}")

q4 = "What is an AI Agent?"
print(f"Q: {q4}")
print(f"A: {answer_with_search(q4)}")

"""### ğŸ‰ Summary

In this notebook, you:

âœ… Gave your AI **real-time knowledge**

âœ… Used `DuckDuckGoSearchAPIWrapper` (no API key!)

âœ… Built a search-augmented AI

ğŸ’¡ This is **Retrieval-Augmented Generation (RAG) with live data** â€” used in enterprise AI systems

â¡ï¸ **Next: Make AI use tools â€” become a true agent!**

### ğŸ”— Resources

- [LangChain Tools Docs](https://python.langchain.com/docs/modules/agents/tools/)
- [DuckDuckGo Search Wrapper](https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.duckduckgo_search.DuckDuckGoSearchAPIWrapper.html)
- [Author: Doug Ortiz](https://www.linkedin.com/in/doug-ortiz-illustris/)
- [YouTube Channel: @techbits-do](https://www.youtube.com/@techbits-do)
- [Illustris.org](https://www.illustris.org)
"""