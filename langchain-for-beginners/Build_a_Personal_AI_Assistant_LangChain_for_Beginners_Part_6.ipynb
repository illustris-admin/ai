{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab-1"
      },
      "source": [
        "### ğŸ“˜ Video 6: Build a Personal AI Assistant (LangChain for Beginners, Part 6)\n",
        "\n",
        "**Goal**: Combine memory, tools, and smart decisions into one powerful AI assistant.\n",
        "\n",
        "âœ… No API keys\n",
        "\n",
        "âœ… Uses Hugging Face + LangChain\n",
        "\n",
        "âœ… Runs on free Colab GPU\n",
        "\n",
        "ğŸš€ You're now building real AI apps â€” just like Doug does at Illustris!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# Install only what we need\n",
        "!pip install -q transformers torch accelerate bitsandbytes duckduckgo-search pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "load-model",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "950e0409-2048-472d-8cee-f4a138776286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=150,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "def llm(prompt):\n",
        "    return pipe(prompt)[0]['generated_text'].replace(prompt, '').strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add-tools"
      },
      "source": [
        "### ğŸ› ï¸ Define Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tools"
      },
      "outputs": [],
      "source": [
        "from duckduckgo_search import DDGS\n",
        "\n",
        "def search_tool(query):\n",
        "    with DDGS() as ddgs:\n",
        "        results = ddgs.text(query, max_results=3)\n",
        "    return \"\\n\".join([r['body'] for r in results])\n",
        "\n",
        "def calculator_tool(expr):\n",
        "    try:\n",
        "        return str(eval(expr.replace(' ', '')))\n",
        "    except:\n",
        "        return \"Error: Invalid expression\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "manual-memory"
      },
      "source": [
        "### ğŸ§  Manual Memory (Simple & Reliable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "memory"
      },
      "outputs": [],
      "source": [
        "# Store conversation history manually\n",
        "chat_history = []\n",
        "\n",
        "def add_message(role, content):\n",
        "    chat_history.append(f\"{role}: {content}\")\n",
        "\n",
        "def get_context(window=4):\n",
        "    return \"\\n\".join(chat_history[-window:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agent-loop"
      },
      "source": [
        "### ğŸ§  Agent Logic: Decide â†’ Act"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "agent"
      },
      "outputs": [],
      "source": [
        "def ask(question):\n",
        "    context = get_context()\n",
        "\n",
        "    # Prompt with memory and decision logic\n",
        "    prompt = f\"<|im_start|>system\\nYou are a helpful assistant.\\nIf the question needs current data, reply 'USE_SEARCH'.\\nIf it needs math, reply 'USE_CALC'.\\nOtherwise, answer directly.<|im_end|>\\n{context}\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "    response = llm(prompt).lower()\n",
        "\n",
        "    if \"use_search\" in response:\n",
        "        print(\"ğŸ” Searching...\")\n",
        "        context = search_tool(question)[:1000]\n",
        "        final_prompt = f\"<|im_start|>system\\nAnswer using only:\\n{context}<|im_end|>\\n{get_context()}\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "        answer = llm(final_prompt)\n",
        "        add_message(\"user\", question)\n",
        "        add_message(\"assistant\", answer)\n",
        "        return answer\n",
        "\n",
        "    elif \"use_calc\" in response:\n",
        "        print(\"ğŸ§® Calculating...\")\n",
        "        expr = question.replace('what is', '').replace('?', '').strip()\n",
        "        answer = calculator_tool(expr)\n",
        "        add_message(\"user\", question)\n",
        "        add_message(\"assistant\", answer)\n",
        "        return answer\n",
        "\n",
        "    else:\n",
        "        answer = response.replace('use_search', '').replace('use_calc', '').strip()\n",
        "        add_message(\"user\", question)\n",
        "        add_message(\"assistant\", answer)\n",
        "        return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demo"
      },
      "source": [
        "### ğŸ§ª Test Your AI Assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-assistant",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b639dca2-48c4-46a6-fc81-fd92578c283b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ™ï¸  Welcome to Your Personal AI Assistant!\n",
            "\n",
            "ğŸ§‘â€ğŸ’» You: My name is Alex. I like hiking.\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ™ï¸  Welcome to Your Personal AI Assistant!\\n\")\n",
        "\n",
        "print(f\"ğŸ§‘â€ğŸ’» You: My name is Alex. I like hiking.\")\n",
        "print(f\"ğŸ¤– AI: {ask('My name is Alex. I like hiking.')}\")\n",
        "print()\n",
        "\n",
        "print(f\"ğŸ§‘â€ğŸ’» You: What's my name?\")\n",
        "print(f\"ğŸ¤– AI: {ask('What\\'s my name?')}\")\n",
        "print()\n",
        "\n",
        "print(f\"ğŸ§‘â€ğŸ’» You: What is 15 * 25?\")\n",
        "print(f\"ğŸ¤– AI: {ask('What is 15 * 25?')}\")\n",
        "print()\n",
        "\n",
        "print(f\"ğŸ§‘â€ğŸ’» You: Who won the 2024 Eurovision Song Contest?\")\n",
        "print(f\"ğŸ¤– AI: {ask('Who won the 2024 Eurovision Song Contest?')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "### ğŸ‰ Final Summary\n",
        "\n",
        "You've built a full AI assistant with:\n",
        "\n",
        "âœ… **Memory** â€“ remembers conversations\n",
        "\n",
        "âœ… **Tools** â€“ searches the web, calculates\n",
        "\n",
        "âœ… **Agent logic** â€“ decides when to act\n",
        "\n",
        "ğŸ’¡ This is the foundation of **enterprise AI systems** â€” taught simply, freely, and powerfully\n",
        "\n",
        "â¡ï¸ **Want more?**\n",
        "\n",
        "ğŸ“š Enroll in Dougâ€™s course: [Leveraging RAG with PostgreSQL](https://www.illustris.org/courses)\n",
        "\n",
        "ğŸ¥ Subscribe: [YouTube @techbits-do](https://www.youtube.com/@techbits-do)\n",
        "\n",
        "ğŸ”— Connect: [LinkedIn /doug-ortiz-illustris](https://www.linkedin.com/in/doug-ortiz-illustris/)\n",
        "\n",
        "ğŸ’¾ Code: [github.com/illustris-admin/ai/tree/main/langchain-for-beginners](https://github.com/illustris-admin/ai/tree/main/langchain-for-beginners)\n",
        "\n",
        "Thank you for learning with Doug Ortiz â€” let's build impactful AI together!"
      ]
    }
  ]
}