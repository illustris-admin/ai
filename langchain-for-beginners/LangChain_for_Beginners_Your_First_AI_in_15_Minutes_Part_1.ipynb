{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab-1"
      },
      "source": [
        "### üìò Your First AI in 15 Minutes (LangChain for Beginners, Part 1)\n",
        "\n",
        "**Goal**: Install LangChain, load a local model, and run your first prompt ‚Äî all for free in under 15 minutes.\n",
        "\n",
        "‚úÖ No API keys\n",
        "\n",
        "‚úÖ Uses Hugging Face + LangChain\n",
        "\n",
        "‚úÖ Runs on free Google Colab GPU\n",
        "\n",
        "üöÄ Let's go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q langchain-huggingface transformers torch accelerate bitsandbytes sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "why-local"
      },
      "source": [
        "### ü§ñ Why Use a Local Model?\n",
        "\n",
        "- üîê **Private**: Your data stays in Colab\n",
        "- üí∏ **Free**: No API costs\n",
        "- üõ†Ô∏è **Control**: You own the model\n",
        "- üß† **Educational**: Learn how LLMs really work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-model",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f307a447-45c8-40b6-d73a-a72358b5bc35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded and ready!\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# Step 1: Choose a lightweight, fast model\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Create text generation pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Wrap with LangChain\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(\"‚úÖ Model loaded and ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "first-prompt"
      },
      "source": [
        "### üß™ Run Your First Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-prompt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995ba38c-dba9-46d3-ec00-039b4c1dff43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Explain quantum computing in simple terms.\n",
            "\n",
            "A: <|im_start|>user\n",
            "Explain quantum computing in simple terms.\n",
            "Quantum computing is a type of computing that operates using quantum mechanics. Unlike traditional computing, which is based on classical physics, quantum computing relies on the principles of quantum mechanics to perform calculations and process data.\n",
            "\n",
            "In simple terms, quantum computing involves the use of qubits, which are quantum bits (also known as \"quantum bits\"). Qubits are represented as \"1\" or \"0\" bits, but they can exist in superposition, meaning they can exist\n"
          ]
        }
      ],
      "source": [
        "# Improved: Use chat template and clean output\n",
        "def ask(question):\n",
        "    prompt = f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "    response = llm.invoke(prompt)\n",
        "    # Clean up response\n",
        "    answer = response.strip().replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
        "    print(f\"Q: {question}\\n\")\n",
        "    print(f\"A: {answer}\")\n",
        "\n",
        "# Try it!\n",
        "ask(\"Explain quantum computing in simple terms.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "try-more"
      },
      "source": [
        "### üîÑ Try More Prompts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "more-prompts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d22c6c-dfdd-4fb2-eaa5-5c74c5a7c9d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Write a haiku about rain.\n",
            "A: Rain's embrace, a gentle hug\n",
            "A refreshing break, a divine blessing\n",
            "The world comes alive, a symphony of colors\n",
            "A moment of peace, a reminder of love\n",
            "\n",
            "Q: Write a Python function to reverse a string.\n",
            "A: Here's a Python function to reverse a string:\n",
            "\n",
            "```python\n",
            "def reverse_string(s):\n",
            "    \"\"\"\n",
            "    Reverse a string using the slicing operator.\n",
            "\n",
            "    Args:\n",
            "        s (str): The string to reverse.\n",
            "\n",
            "    Returns:\n",
            "        str: The reversed string.\n",
            "    \"\"\"\n",
            "    return s[::-1]\n",
            "```\n",
            "\n",
            "This function takes a string (`s`) as input and returns a revers\n",
            "\n",
            "Q: Tell me a joke about AI.\n",
            "A: Once upon a time, there was a robot called Alex. Alex was great at making jokes, but not so good at telling them. One day, Alex was asked to tell a joke, and he knew just what to say.\n",
            "\n",
            "\"I am a robot. I'm programmed to make jokes. I've been making jokes all my life, but they don't always come out right. But today, I'm going to tell the funniest\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the clean ask() function\n",
        "def ask(question):\n",
        "    prompt = f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "    response = llm.invoke(prompt)\n",
        "    answer = response.replace(prompt, \"\").replace(\"<|im_start|>assistant\\n\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {answer}\\n\")\n",
        "\n",
        "# Try fun prompts\n",
        "ask(\"Write a haiku about rain.\")\n",
        "\n",
        "ask(\"Write a Python function to reverse a string.\")\n",
        "\n",
        "ask(\"Tell me a joke about AI.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "what-is-langchain"
      },
      "source": [
        "### üß© What Is LangChain?\n",
        "\n",
        "> **LangChain** is a framework for building applications powered by large language models.\n",
        "\n",
        "üîß It helps you:\n",
        "- Connect models to data\n",
        "- Add memory and context\n",
        "- Chain prompts, tools, and actions\n",
        "- Build chatbots, agents, and workflows\n",
        "\n",
        "Think of it as **Lego blocks for AI apps**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup"
      },
      "source": [
        "### üßπ Optional: Free GPU Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup-gpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b5b210-322e-4e66-b865-db9435a63fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory cleared.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "print(\"GPU memory cleared.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "### üéâ Summary\n",
        "\n",
        "In this notebook, you:\n",
        "\n",
        "‚úÖ Installed LangChain and Hugging Face libraries\n",
        "\n",
        "‚úÖ Loaded **TinyLlama** ‚Äî a free, local, private model\n",
        "\n",
        "‚úÖ Wrapped it in LangChain\n",
        "\n",
        "‚úÖ Ran real prompts ‚Äî no API key needed\n",
        "\n",
        "üí° You now have a working AI assistant in Colab ‚Äî in under 15 minutes!\n",
        "\n",
        "‚û°Ô∏è **Next: Add memory so your AI remembers the conversation!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "resources"
      },
      "source": [
        "### üîó Resources\n",
        "\n",
        "- [TinyLlama on Hugging Face](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n",
        "- [LangChain Docs](https://python.langchain.com)\n",
        "- [Hugging Face Models](https://huggingface.co/models)"
      ]
    }
  ]
}